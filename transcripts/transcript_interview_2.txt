Transcript: Developer Interview #200:00:00 A: Currently my position is technological lead in [hidden for double blind], the airline company. So, as a tech lead, I am I'm leading a development team composed of 5-7 developers. So mostly we are working with Python and SCALA applications. Before this position, I mainly worked in startups in Berlin for a while and Italy for some years and my experience has been always in the field of software engineering. So, I've got a lot to do with tests.00:01:03 Q: You have come around a lot. Also, how many years of experience in development do you have approximately?00:01:18 A: Approximately seven years. It's been a while. [hidden for double blind]00:01:46 Q: First, I would like to talk about the process of creating tests and maybe your practices.00:01:55 A: Yeah.00:01:56 Q: So maybe first it's one of the classic questions: How do you see the scope of the test or when I say test case, we always mean unit tests. So how do you see the scope of a unit test? Should it always test exactly one thing, or do you try to cover as many scenarios as possible in a test case?00:02:31 A: What I generally try to do myself, but also what I try to teach to maybe junior developer that joined the team, is to ... when approaching unit testing you really need to divide your test cases into a single scenario-/single branch-testing. So generally what I always do, if I have to test let's say a stack, so I don't generally go directly for the main behavior of a stack, which is like push, pop and then you should get the last element. But what I generally do I start with testing first the unhappy part. So, like try pop without pushing anything. And then I try to test another branch of the unhappy part. And then when I have these tests set up, I go for the goal and I test the actual behavior that I am building.00:03:53 Q: Ok, so is there a reason as to why you test the unconventional parts first? Because obviously these are the more interesting tests. But is there some reason for it?00:04:18 A: Yeah. And the main reason for it is because this is a good way to do test driven development. And if you want to do test driven development, if you start directly with the behavior that you're trying to test then you get that one in and you don't focus on the unhappy part so you only basically test the happy part. And everything works fine, but then you release it to production, and you have tons of bugs because you didn«t think about the unhappy part.00:04:51 Q: Ah, so you also follow the test-driven development approach?00:04:58 A: Yeah. We try to follow it as much as possible. And yeah, it's not always easy. Especially for junior developers because for test driven development you need to have some experience with it, on the first times you try to do it it's difficult, as it's not really a good experience but after a while it's really easy to do00:05:34 Q: I see, because you focus first on the frustrating part of your code so maybe because of that.00:05:43 A: Yeah. Yeah. In general, test-driven development is really helpful when you reach a point where you cannot run the application, because it's going to develop a script or like a very simple application. What the developers generally do, they just execute the application and modify the source code and then execute again and they this process of like ping-ponging between execution and source code modification is not possible way when you're developing a complex application that needs to run in a cluster and needs to do many things.00:06:27 Q: Yes, and it may take a lot of time.00:06:30 A: Exactly.00:06:34 Q: So, do you do you always focus on getting 100 percent of the branches covered in production methods? Or do you settle with maybe the 90 percent that are most common?00:07:01 A: That's a good question. It generally depends on the project. So, if we are building a critical system, then you should have a decent branch coverage, because I mean it's a critical system. So, you really need to test it very well. And in that case, branch coverage it's not really a good metric of how good your test suite is, because I can easily have 100 percent test coverage but my test cases, they all suck.00:07:50 Q: Yes.00:07:53 A: And, therefore. We try to use it internally. Like for the developer themselves you know just to have a hunch of how much am I covering. But this metric should always remain private, you should never let management know about this metric. They don't understand it and they just believe oh wow this is 100 percent branch coverage, so it means it should work and we have no bugs anymore. So, we use it for some projects, and I've seen some limits set to 80 percent, some limit set to 50. But it becomes very tricky. So, I don't know if I answered your question.00:08:54 Q: Yes. But then again it may depend on what exactly are testing.00:09:02 A: Yeah but it's much more like guidance to the developers.00:09:08 Q: Yeah. So, what else do you use to assess your test coverage or your test cases that you make sure to cover as many things as possible of your production code? Do you use any other indicators to assess that?00:09:32 A: What do you mean?00:09:34 Q: For example, you said that you use branch coverage as a guidance. Do you look at other metrics? Or do you have other testing libraries that you use to assess the completeness of the tests?00:09:53 A: No, not really because our main focus has never been critical systems. So testing is not... we don't have enough resources to dedicate to testing in the first place. So, the only metrics that we use is coverage, specifically branch coverage with different libraries or different technological stacks, like for SCALA and Python that's the main metrics that we use. You were talking about these, right? Not like coming up with what to test?00:10:44 Q: Yes, metrics or also testing libraries like maybe... we use for mutation testing we used PITest or things like that.00:11:04 A: Yeah, no. And especially also because most of the technologies/the tech stacks that we work with don't have that good support for these kind of libraries in the same way that maybe Java or C might have.00:11:35 Q: OK and maybe what is your procedure when you see that a test case suddenly fails? What do you usually look at or where do you aim for changes? Do you look at production code or do you also look at a test case?00:12:03 A: Generally, the first thing I do is I look at the test case that fails, because the test case generally depicts how the production code should be used. And also, the test case is also code. So maybe the test case is wrong or maybe the test case itself is a fragile test case and then we introduced a new feature and of course the test breaks now. But in general, I give a look at the test case first and make sure that that's the behavior that we want to have. And if the test case is solid and it's OK, then I hop over to the production code or modify it to match the new behavior that I want to have.00:13:32 Q: Maybe now we could have a look at what you think an efficient test should include? Or what an efficient test is according to your opinion?00:13:45 A: Sure.00:13:46 Q: So first of all, how would you define the effectiveness of a test? In the literature it's defined as how many faults a test case can find, which depending on your strategy of how you create test cases/scope won't really be an indicator for effectiveness because at best a test case may only can detect one exact fault. So how would you assess an effective test case?00:14:50 A: I do agree with finding fault in your code, but for me the quality of a test case, it's also regarding its fragility, its reproducibility maybe some test case is likely to do to fail even though the production code is not impacted, but it's because there's a fault in the test case itself. So that's also would be an indicator of the quality of a test case. But in general, for me a test case that's effective is a test case that tests one single scenario/instantiation instance of a behavior that we want to add. So if I want to add feature A, then I have the happy path for feature A and then a bunch of unhappy parts for the feature it said. And how you come up with these different test cases, it can be up to many factors. How much time you have, how much critical you want to be and how much time you have to apply techniques as to boundary testing ... these kinds of things.00:16:47 A: But for me a test case is effective when it tests one specific scenario instance of a behavior. And yeah ... if it stays there, so the richer the suite is, the more faults you can find.00:17:11 Q: Do you also look at maybe the readability of test case?00:17:27 A: Yes.00:17:27 Q: So do you have typical practices where you have a common naming for the test methods?00:17:35 A: Yes. Readability is very much important for us, especially in test cases, even more than production code, because otherwise it becomes too tricky to write test cases because writing a test case is in general harder than writing production code. For instance, when we work in the aviation industry, so we deal with schedules a lot, like you know a flight is performed by this aircraft at this time. So we have complex objects, right. So if you want to test specific behavior you need a schedule, but if you want to have a schedule you need to create a schedule. You need to instantiate an object which is a complex object and then you end up having a test picture that's a very big chunk of code that you need to repeat over and over again. And this becomes very tricky and not readable. So for me a test like this is not effective. It«s not really maintainable.00:19:09 Q: Okay so you would say that a test case that has a lot of initiation process, that needs to create all these complex objects will not be effective for you?00:19:25 A: Yeah. It would not be effective as it's hard to maintain. And it's also difficult to understand, what if this test failed then you don't know why it failed, because you need to read 200 lines of code of input data to understand why it failed. And for me it's not effective because you are not able to pinpoint the problem and that's what a unit test is for.00:19:59 Q: Yes. Do you use any coding practices to make sure that maybe certain precondition or post condition are met? ThatÕs in the production code.00:20:37 A: You're talking about the production code?00:20:49 Q: Yes00:20:52 A: We do apply sometimes both postconditions and preconditions, but in a very limited way because if you use especially statically typed language, the type system is already helpful in that regard. So, it's really a kind of pre-/postcondition, at least with types. Specifying more pre-/postconditions in that scenario is like a little bit too much. And in general, it's not really good for readability purposes. We experimented with it a little bit, for instance, imagine you're dealing with a lot of CSV files and then we experimented with setting a kind of pre and post conditions for validation of the CSV data format. So, this column must contain all integer, this column must contain all float and then you input something and then the precondition is verified and when you return it, the post condition must be verified. So, the developer cannot change the content of that column. He needs to respect like the number of columns or this kind of stuff. Yeah but in the end, we didn't find it very useful.00:22:43 Q: So, do you also have certain techniques where you assure that the test cases are behaving correctly? Or do you always try to design them as simple and short as possible so that they don't actually have any behavior on their own for any additional behavior?00:23:10 A: Yeah. The second one. We generally try to be light regarding the input preparation part. So, the subject, let's say you want to test a function and this function has a bunch of parameters and gives you an output. You want to test the output against a couple of parameters. And what we generally try to do is, we try to separate the parameters from the test case itself. So, in this way to get the test case really contains the behavior that you want to test. And you can execute the same test case many times with different kind of different parameters, but the parameters creation is kind of in another stage. So, your Readability is increased.00:24:37 Q: How do you usually find the faults when a test case fails? How do you locate the fault or what helps you locate the default in a test?00:25:07 A: Do you mean the naming of the test case?00:25:09 Q: Yes, if the test case fails. How do you know what exactly failed in your production code? What indicates it for you?00:25:21 A: Well when a test case fails, in general you have a reference to the test case name, so you can hop in directly to the test case and then in the test case you have the subject under test, which might be a class or function and then you just need to make sure which code is failing, you get the input that you're testing and you're already know what's the input. Then you hop over to the production code and you just read a production code and you can find the fault. So that's the general procedure that we use.00:26:32 Q: Ok. To summarize the few points that we said that makes for an efficient test: The first point was that it needs to find the fault that it is designed to find and the versatility, the reproducibility, we had the correct behavior and that if should test one scenario, the readability and maintainability. And do you maybe have something else that indicates the effectiveness? Maybe you do also look at certain tests smells during the development?00:27:33 A: Yeah. In general, we also look at the correct way of testing. As a matter of fact, we initially, with some junior developers we had some issues at the beginning when we did our first tests being pull requested, because the tests were full of test smells and they weren«t really good. So that's definitely something I give a look at when I get a pull request for a new test case. One part we especially look at is the fragility of a test. So that's a good example. Everything runs fine. And then somebody introduces a code and suddenly 40 test cases just fail. That's an indicator of a fragile test suite because I mean if there is a fault then one two maybe three test cases should fail, but not like a majority of the test suite for that specific combo.00:29:35 Q: Would you say that these factors are all equally as important or would you say that maybe the readability is the most important one or should be more important than some of the others?00:29:54 A: Well definitely, I do see an ordering among them. Readability is certainly very important, in order to have a good test suite. Yeah, they're also very interconnected between each other, because a readable test case is of course a test case that's simple, that tests one specific scenario, that doesn't contain test smells, so they're all interconnected. But I would say finding the fault is important, then readability is important than the rest kind of follows as a consequence, or like a minor factor contributing to the quality of a test case.00:31:16 Q: [...]00:31:59 A: Definitely, the thing is, writing these kinds of tests, like really good, clean tests that can be your documentation is really hard and ideally everyone would want to be in that spot but it's very difficult to achieve.00:32:26 Q: Especially if you maybe don't follow a test-driven development from the start. Then you're kind of always running after that.00:32:38 A: Yeah especially if your team is not mature enough so you work mainly with junior developers, these things take time and experience to master.00:33:01 A: So would you be able to assess the effectiveness of a test based on the named factors? Could you look at your tests that you wrote and can exactly tell which tests are effective and which not?00:33:29 A: Well, sure you can give a degree of effectiveness, but I would still look at the production code. I cannot just rely on the tests. Because there are things such as performance, which can be also a fault, because if you have a requirement that it says it needs to run in under one second... yeah testing that, good luck. So then of course we are talking about unit tests here, so maybe it's a little less important as for Integration tests. But yeah, it's sometimes also unit tests have to consider performance and, in such cases, writing test for such cases can be really hard and can become less readable. So, in the end you might judge the effectiveness as less effective but that's not 100% depending on the test, but it is also depended on the production code. So, I would say that in the end, the degree of effectiveness should always include the production code. So, to answer the question, no.00:35:24 Q: Yeah. I think it's very difficult to generalize these things because it always depends on what it is testing what this test case is validating.00:35:45 A: Yes indeed. And yes, the requirements are really important but. Requirements are volatile and they change and It's very difficult to keep them under version control.00:36:04 Q: Maybe it could be used more as a guidance, not an absolute definition that this is effective and the other is not. It could be used as a guided reference.00:36:28 A: Yes. Yeah, it's true. I agree with that.00:36:36 Q: Yeah. I think that's all for now. Do you have any questions? Or would you like to add something?00:36:51 A: Yeah, I think we could talk for like 3 hours. [wrap up of the interview]