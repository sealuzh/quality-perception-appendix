Transcript: Developer Interview #100:00:00Q: So, first I would like to ask you some background questions about your background. First would be your experience as a developer in general?00:00:21A: That's super hard to answer. I mean, I code as long as I remember so I probably started somewhere in school. It's like for super long. So during my professional studies actually I also had like a little company where I was building applications for applications for from my customers so this was probably the first professional experience. And since then I mean I mean I really like coding so it's not that a lot of people in research at some point like all the code is basically like scripts like sadistic scripts but I also really do like to decode actual applications. So doing my pitch dealing one of one of the major outcomes was actually quite extensive piece of leg and quite a pretty extensive extension for a little studio we were working with multiple people on that were actually quite proud to have like a super extensive test suite. I think we had like 2000 tests also in total all manually crafted.00:01:26A: It's like yeah since I would say yeah I would say I'm experienced. I like coding and yes.00:01:34Q: And so also as an industrial and an appearance you set yourself quite some.00:01:39A: Yeah I mean this is I would put this a bit in and take this with a bit of grain of salt because I mean it's not the day that I was working in the industrial team so far. So it's not that I have leg work with.00:01:53Q: The product teams. So for me it was I mean so I would say big it's the biggest experience of far where a project that we had to implement during my that was actually doing my masters so we basically were working together with a company and a group before and like after this project ended that I actually continued working with them for I think another half a year or so and I would say that was the closest to two to actual professional development and like the rest of my development was open source I would say that it always depends on how you want to different because it's a bit hard to differentiate open source. Did you participate in open source projects. I am. I mean my brought my own projects very open source the idea. I mean I was for some time I was actually eclipse committal. Because one of our open source project is actually in the eclipse in the official eclipse release train. But I mean it's something I didn't have time to continue. Doing that. So.00:02:58Q: So, did the open source project that I was talking about. I mean I was mainly working with my own students and with colleagues. But I mean we had like it I think at the top time we had eight simultaneous people working on the projects that I would say it's and I mean that the project in total in the end we had like three hundred thousand lines of code. And so it did it is a series of projects that. I find open source projects very interesting because you have. Very different.00:03:26Q: Coding styles and ideas that come together. So that's I mean it's I find it hard to draw the line where I mean.00:03:34Q: Where does open source end and where does professional development start or is it actually two orthogonal dimensions. Because I mean also in a lot of open source projects you have the same developers that are also hired by your company but they just contribute to open source so it's yeah. It's probably not the right access to it to differentiate probably it's do you do this professionally or do you do it in your spare time for it yeah. Or. And then I would say yes I get paid for it because they get money for doing their GHG while I was alternatively developing that open source project. Yeah. Okay.00:04:17Q: So next I would like to ask some questions about the process of creating tests and afterwards we would dive into the definition of effective test case. So first off: What do you think should be the scope of a single test case? Or what part should it test of a production code method?00:04:42A: So I'm a big fan of 1-Assert unit tests and I am like a strong fighter for it. So, when I test, typically I would actually start with like a set of black box tests. So, I really like to use the test suit to develop the interface of the component that I'm developing so I would not follow traditional TDD but I would like actually come up with like a super extensive test suite. I do the test suite first because I want to make sure that the component that I'm building is actually designed in a way that I like that I actually want to use as a consumer of the API. And only when I have like already like a good set of the of the tests I would actually start implementing them and then I would say I'd transition more into this TDD cycle where actually once I discover a case that I didn't discover before I would write another test and then I would implement it. Typically, I start with a very extensive test suite before I actually start implementing the contents of the method.00:05:53Q: Do you do this on an API-level or already on unit-level?00:06:35A: No on unit-level, the other would be integration testing at the end.00:06:35Q: So you start with unit testing to develop your application and then you start testing the integration and then maybe go to higher level.00:06:46A: I mean that always depends a bit on what you're actually developing. I'm very often using Mocks for testing, like for simulating this kind of integration test for when I can actually say: Okay, if I use this external component my concurrent component relies on, then I would just like to work with Mocks to mock the behavior that I'm expecting, but I would still just write unit tests. And for the integration tests. So I mean a project that I did during my Phd. I mean we had like tons of integration tests, the problem was that we were actually serializing data that we were collecting from developers and I mean over time of course the software evolved but we need to make sure that we're still compatible to the old serializations because we didn't want to lose data. So we had an extensive set to make sure that we're still able to read the old serialized data. Also for the same application also had for example like a test plan. So I've actually written down things, maybe in a bit too informal way, but I've actually written down things that I've wanted check before release. To make sure to check the following things. Does the following generator still work? Can I still extract the following details if I do the following activity? Do I see this in the Log etc.00:08:19Q: But yeah, I did do this separately from the documentation of your code? Or was this part of the documentation?00:08:31A: Code itself is documentation... I mean it's funny but to some degree I am serious about that sort of thing. I mean you have to differentiate what you actually want to document, and we have the strict policy that classes don't get documented. So, what we documented extensively were interfaces and everything that you actually inherit and cannot see. Because I mean for an interface there is no implementation so you should define the behavior and define the contract and define what whatever subclasses you have to expect, but from the code, I would say, if code needs documentation you should clean it up.00:09:13Q: That's. Yeah. That's true to some degree00:09:17A: Of course, there are always corner cases but in general before I write a comment I think twice about whether I can clean up the implementation before. But admittedly I mean I guess the documentation is the part that can probably be improved.00:09:36Q: So I I ask the initial question about the scope of the test case, because I analyzed some Apache Open-Source projects and I saw that A lot of the test cases were just like one test case for all possible scenarios, with a hundred Asserts in the same test. Well maybe they have some reasoning for it and it would be interesting to know. But When maybe the 100th Assert fails, what does it tell you?00:10:15A: I mean of course I am a bit biased there, but I would say if somebody writes test case like that, the problem is not the software that has to be tested, but the developer that doesn«t know how to write a proper test case. Because I mean if you have a few methods with tons of assertions, then probably whenever you change something, the test will break it is super likely that the test will break all the time, that the breakage won't tell you anything because it's always the same method that breaks and you actually have to look at the code again to understand why it breaks. While if you'd like managed to to put it onto small test cases and with one Assert and just based on the method name you typically know.00:11:03Q: So, you tend to write a test case for each method of the production code or multiple?00:11:10A: At least, at least. If I think about it, I would typically start writing the happy path. to see where I say okay key if I assume that all the inputs are correct and there is no corner case, and these are the test cases that you can easily write upfront And if you just do that, then you would already cover all the methods that are there. And then of course some methods are more complex, then you need to test more corner cases or invalid inputs or whatever and then you would need to add more test cases for that. But definitely every public method needs to be tested because it can be used individually.00:11:50Q: And do you follow a systematic procedure to identify the corner cases? Or the test method that you should write tests for the production code methods?00:12:03A: I mean given that I have these two phases where basically you start with blackbox testing.00:12:08A: I mean based on the domain knowledge, I would try to identify corner cases. Once they have code and I see that there is an IF and I probably didn't test the ELSE and in the end of course I also usually use coverage to see whether I missed something in the implementation, but I would say in the beginning it's more unstructured, because it's basically my domain knowledge that tells me: Ok, these are the interesting cases that I should cover and this is how I want to use the API and does this API work in the following way etc. But more towards the end, there I would say it becomes more structured because I see other. Parts of the method that are not yet covered or do I have like more if conditions that I need to think about more closely because the combinations maybe may make a difference etc.00:13:01A: I have to add, I still think that this counts as a very structured approach. I would say. I think paying attention to test coverage is probably the most structured way to to actually see whether something is covered or not.00:13:18Q: So where do you stop with edge cases? How many edge cases do you try to cover?00:13:26A: All of them.00:13:26Q: Obviously you would always try to get all of them, but so are you happy with 100% coverage?00:13:34A: There is this funny video of Uncle Martin where he says: It doesn't make sense to have a different goal than 100% coverage. I like this a lot actually. I mean of course, for some parts it's actually really hard to achieve good coverage or actually, to test them properly. Especially if you have I/O interactions or like networking or some UI threads that you need to interact with, because they're inherently really hard to test. But what I would try in these cases is always to capsule these cases into like components that are focused basically just on this one problem and then they have a testable component that just relies on this capsule component that it cannot test. So I tried to minimize the things that I cannot test in like separate components and then the separate components I can test them manually in the worst case and probably have a super bad test coverage with this one component. But. I mean at least the big part, the logic part I can test and they would just mock the other components to test it and that makes it testable.00:14:52Q: So you generally look at code coverage to tell how much you should test? Or do you look at other things? Maybe also branch coverage?00:15:04A: The answer is yes, I look at code coverage but for me it's not like 70 percent is enough. So for me, the code coverage is basically boolean flag and I use it to explore the code: Is there something that I should cover that I didn't cover? So I mean there is no other meaningful target than 100% because I want to make sure that I cover all the cases. It's not that I say oh it is 80% tested now I'm happy. I would like to cover everything. That's where I use coverage. Sometimes you really just miss a case where you say, okay, you're right this ELSE statement is not covered in my test cases yet. So then I would come up with a different case to solve it.00:15:50Q: So you don«t believe in coverage as a main indicator of test quality?00:15:52A: Yeah, I think is not as main indicator of quantity. So I said so I don't believe at all in having limits in test coverage for example I had a discussion whether this should be for example a quality indicator for the management student groups. I don't think that makes any sense. Code coverage tool is a tool for the developer that writes the test, but that cannot be used as a management instrument, to say whether that code is good or not,.I mean it's really just a tool for you to see okay. Did I forget anything. Great. Everything's fine, green. You know, that's at least how I see code coverage.00:16:40Q: So basically it's just a guidance.00:16:42A: Yeah. I mean it's quite easy to come up with 100 percent code coverage. I mean you don't need a single the search for that. That's the nice thing.00:16:50A: So the point is coverage only tells you whether the code was executed, but not whether your tests are meaningful. And that's the main thing that people need to realize. It doesn't say anything about the quality of your tests. At least that's my opinion.00:17:16Q: Do you also use any other indicators, such as mutation coverage? Or other things?00:17:24A: Well I have not used that before.00:17:27Q: Is there a reason for that? Is there a lack of thoughts? Or you don't believe that mutation coverage is helpful?00:17:34A: No I actually never thought about. It's a pain. I didn't understand so far about why I would actually use it. So that's it's. I mean, it would probably be an interesting task to try it out and see how good my tests actually are. No I have never mutated like anything.00:18:12Q: Or maybe you just did manually flip a statement and look how many tests that fail? 00:18:21A: But that's the point, I mean you get that for free right. Because I pay attention to seeing every test fail at least once and as long as you only have one a Assert per method, then it's quite straightforward to see if i am testing the right thing. That gets much more complicated if you have 20 Assert in one method because then, yes, the method fails, but it can fail for like a plethora of reasons. And you always need to step through, to see why it fails.00:18:55A: But I think that's crucial. I mean you need to see a test fail, to make sure that it actually points out something and then, only then you can continue from that. So I would expect that if I would take my implementation, then I would have like actually quite a high [mutation] coverage. But yeah. And. I don't know. I never tried it.00:19:16Q: So what is the general procedure, when you have a failed test after doing some modification to the system under test?00:19:41A: I mean just deleting the test would mean that that would only like remove the symptom but not the problem itself. I mean it's always the question, when does it fail. It might be the case that when I write a test case that the test case can be wrong. At the same time, I mean if you write code you might discover something like an incorrect test expectations. So yes it's not that easy to say every time a test fails it's the test problem or every time it fails it«s the implementation problem, I mean it can be both in theory. For me typically, I mean I try to understand why the test failed. A typical reason is that the system changed the way that basically the requirements changed. You say you get different expectations and the former test expectation doesn't hold. But of course, it can also be the case that you really discover implementation issue and then you need to fixed the implementation.00:20:50Q: Do you have an approach to check whether you're in test behaves correctly?00:21:02A: Debugging. I mean that there is no one no golden procedure that you can do. It's typically just stepping through the code. I mean that's the nice thing about having isolated test cases you can just debug one test case within like two or three steps and you're actually at the point that you're interested in checking whether what's going on or what went wrong. So yeah, I would just use the debugger to see is the result correct and if not. Typically, the only thing that you can debug is actually the implementation to see whether the implementation makes sense or I mean you know whether you just touched it or whether you change something in the implementation and then it's really more an exploratory procedure. I mean what helps significantly, is having proper data structures where you actually can encapsulate different values and say also have like a proper hash code and a proper equals implementation for your data structures makes it a lot easier to identify the test results.00:22:14A: [discussion on testing frameworks]00:31:42A: I think the tests are more important than production code. For example, the code ranges that we did in that project. We mainly talked about testing. Because as long as the testing makes sense, I trust that the implementation works. And then they would basically skim through your cases and isn«t too complex to implement it. But I mean we would mainly review test code. Because at the end, that's basically your specification right. That's my specification, that's where that's the documentation that we had in our project. So that's why I said I mean code essentially comments itself. You have your test suite for that, because the tests will tell you what to do and if you don't know what your code is doing then you write another test case to explore it.00:32:30Q: So, what do you look for in your tests, when you say you do code review on your test code?00:32:40A: That's a good question and that's actually not that easy to answer. So, I mean the first thing that I would look out for were like these monsters that you've mentioned before, like the test cases with 20 asserts. So, at the end what was important for me is that tests need to be readable. And then therefore they need to be short because they should test one thing. One thing that I'm a big fan of is have proper, I'm not sure whether you can actually call them test fixtures but and I like to have set up methods. Because typically, the most complex thing in the test is setting up the environment to the point where you can actually test, whether the next call is what you want to observe. So, I typically try to keep this set up part as expressive as possible by either having setup methods that you can parameterize or by like reusing the setup to some degree. But then again, I mean it's always there's always the trade-off of being like to specific or being too abstract. So, I guess for a test that's important to be specific. So, I don't want to have this super enormous set of set up, that sets up like everything. And then having a test that is testing only like a small part of the component, so I'd like to be the setup also as minimal as possible because that makes it again easier to maintain in the future.00:34:11A: So, this would be like definitely one thing that I would pay major attention to is how readable and maintainable the tests are. And then, this is probably debatable, for me the naming of the tests is important because I mean of course we can write the test cases in a very natural language way and describe exactly what you want to test. But the point is that also makes it at the same time quite hard to identify and read test cases. Like if you should say I change something in a particular method, I would like to try to run all the tests that are related to the method so that's it. Probably if they're just natural language method names probably makes it hard to identify the right methods to find again. In our case, we typically deal with some naming convention for testing methods that at least start with. I don't know. the name of the call method that's being tested. And then afterwards the description of what you actually want to test. I would say we don't use the most natural method identifiers, but we would choose the identifiers in a way that makes it easier to match them with the part of the code that was tested.00:35:29A: What else? I mean of course, I would try to identify whether like the domain is probably reflected in the tests. First of all, of course the test expectations correct. So, I mean we would see whether the inputs is valid or the setup is valid and whether the expected output is correct. I guess that goes without saying correctness was also immediately the main concern. But then of course you also want to make sure that you cover everything, that's probably the hardest part to say. When did we miss something? Is it statistically exhaustive? Do we need to test for other things? Because you never know that because the point is, you can make sure that you have 100 percent test coverage. But the point is, you don't know whether your implementation actually covers all the things that can happen, maybe the implementation is not yet complete and then the 100% coverage doesn't tell you anything, it just tells you that declarative limitations were tested, but it doesn't tell you whether the implementation is finished basically.00:36:44Q: So how important do you think all these factors that you named are? Do you think they are all equally as important? Or do you think some may be?00:37:16A: So, I would differentiate. I mean I would say that of course, correctness I would say is the most important one. I mean you want to make sure that the software works as intended. So, I mean correctness is definitely the most important one. But I would say the other ones definitely correlate with it because it's, I mean sometimes it's just not possible to write nice test cases, especially if you work with legacy code or if you have to reuse other code or maybe some parts that are not easy to test, then the tests will be ugly and there is no way around it, but you still want to make sure that they're like correct.00:37:50A: But especially if you're talk about long term projects, if it's not just about a project that you work on for two months but probably for three years, then maintainability is an issue, especially for long list components I would say also maintainability is like a big task.00:38:11Q: Do you think that the maintainability of a test changes over time? Is it like something important that you should ensure it from the beginning? Or does it stay the same?00:38:26A: So, the fact is, if you say that the test is basically documentation of the component that it tests, then you will maybe get a new developer that works in the project in two years. And even though, there is no change in that piece of code, the developer might want to understand how this piece of code is working, so he would read your test case. So, I guess a proper test suite should is timeless, and therefore probably maintainability is a bit misleading if you say we subsume it under maintainability. Because it also means if you need to change the component, it also gets easier, because you need to change the test again, but it might also be better if the test actually will not change in the future. It's really just in the person that wants to understand what it tests or what the correct implementation should work. And that's. But still, I would say of course correctness is the most important thing, you want to make sure that you test the right things and that you covered the whole spectrum of possible inputs. These are the two most important things.00:39:45Q: Do you think that an effective test, which includes all these metrics that we just named, has a special impact or benefit in different aspects of work?00:40:25A: Based on talks that I have seen so far, typically the effectiveness is somehow related to the ability to identify bugs, basically. Then you could say, maybe a test is more effective, but unfortunately you can also identify this post-mortem, to identify a bug. So, if you find a failing build that was caused by a failing test and apparently this test was good, because it caused something that the developer didn't think about. Then of course, you can also use the mutation testing as a key to effective test identify more cases than others.00:41:15A: I think the problem is the term effectiveness. So of course, naturally I would say some test cases are more important than others. So, I would say, at the very beginning I said activity start with the "happy" path, which is the basic usage of a method. And I mean of course that's the essential thing, if this test case doesn't work, none of the other cases make sense.  If you have special non-functional requirements that are very important for you like performance or security or whatever, then of course other tests might also be very important for you. If you check that, I don't know the password doesn't leave the component or whatever. But I guess that's domain specific probably.00:42:14A: So naturally I would say yes that some tests are more important than others or more effective than others. But I have a problem in like quantifying or explaining the concept of effective. That's the problem, that it's just too ambiguous in my mind, because it depends on the context and probably also in what you're interested in getting out of the test, because I mean you can also say, the main goal is to use test as documentation, then an effective test case contains a lot of comments that explains the individual steps. So that's not ultimately true. I mean if I'm interested in only identifying 14 implementations, then the test cases should identify 14 implementations, if my goal is to have maintainable test cases, then the test cases should be short. And the answer to the question: Is it effective? Depends on the goal. So, I guess that's probably related. So therefore, I cannot give an absolute answer.00:43:22Q: If you if you would expand the concept of an effective test case, beyond the concept of how many faults it detects. What metrics would you add to the concept of test case effectiveness? For example, if you value the readability in your tests and you therefore use a specific name for them, is it also a characteristic of an effective test?00:43:48A: That«s an interesting question. Maybe it is, because I would say that the method names effects basically the traceability I would say, because one of the issues is that if you have like small tests you pre-emptively end up with tons of them. And once you have like 80 tests or so for a class, then it's really actually the challenge to find the right test for your change. So that's why I would say yes, maybe even trivial things like the method naming conventions are important. But I'm a bit hesitant there, because the naming conventions are not like super, uber defined in our case.00:45:11Q: But would it influence your perception of an effective test case if you would have a test case that fits your naming convention and one that doesn't fit it?00:45:28A: Yeah. I mean that's for sure. I mean if I compare to a random name, then I will say a test case is more effective if it contains a reference of the name or if it explains what it does. Because that is important for a developer to understand. Otherwise I mean if you just get random names then I mean if the test case doesn't tell you anything and it«s zero documentation, you basically need to crystallize this from having a look at the test case to see whether it's just random things or whether it actually has a meaning when the test case fails. Other things ... As I said before,  it always depends on the code and I mean if you define the goal such that the test should help you design the system or writing the test case should help you design the system.00:46:22A: How would you quantify that before? You write the test case, then you realize that the existing designers cannot test that and it is also just the perception of how the API is to be used. And if you don't like the way an API is to use, then you would change it. And subsequently you'd also done the test case and then writing the test case serves its purpose that it was basically this version of the test case that was there before that you then afterwards also adapted, that was the valuable test case.00:47:00A: It's a challenging. I would also say the process of writing the test case is very valuable and if you only want to judge the quality of the test after the whole process is finished, then you won't catch some properties.00:47:15Q: That's an interesting point.00:47:24A: But I guess the term of an effective test case is not only subjective but also very depends on the system. I guess I wouldn't say that it's subjective. It just depends on the goal. I think once you have a clear goal in mind it's easy to identify whether test case helps you reach that goal. But without that, then it's just super generic. And then some goals might contradict each other. So if you say you want to have maintainable test cases or short test cases, then I mean this is a different goal than saying you want to have a strong documentation or if your main focus is to identify faults or a typical way is if you want to refactor a legacy system, then I mean you can generate tons of tests for the existing system to make sure you cover the existing behavior. And then once you start refectory it, you can make sure that you only have like behavior preserving changes and then it makes sense to have as many tests as possible and it doesn't matter how they look, as long as they're effective in identifying behavior changes. But then again, these test cases have the only goal of identifying changes to the behavior. You probably cannot use them as documentation. They're probably not even readable, they probably don't report any kind of behavior. They didn't help you find any design issues with the system, because they're basically based on a done component.00:49:05A: So, they're very effective in fulfilling the thing that they're supposed to do, but they're very ineffective in other ways.00:49:19Q: I guess you also couldn't define an effective test only based on the ability to find faults in the production code. Because if you only write short tests, then you may only catch exactly one fault00:50:02Q: So, what do you think about test smells for the effectiveness of a test?00:50:20A: So, I I'm not aware about test smells. I can imagine what it means and I'm not aware of any like manifestations of that so I don't know any literature about that , but I would say many things that I mentioned before would probably count as test smells like having like tons of Asserts in one method. I'm pretty sure there is a smell for that.00:50:40Q: Yeah.00:50:40A: I would even say that even if you only have one Assert and you have a very long  test, maybe because you needed to set up the environment for it extensively, then this is also not a good idea, because readability and understandability suffers from that. You could say this is probably a smell. Many of the things that I have described are probably smells, because ultimately the test might still be valid, it might still find errors, it might still be a perfect test but it's a bit smelly because it hints at problems, maybe in the future, because if you don't understand the test, then maybe you don't realize that a specific change in the implementation class would have also affected the test and they somehow coevolve independently.00:51:32A: Yeah, I would say probably test smells and whatever we look in the code review are good terms for that. 00:51:48Q: What about flakiness? You take some actions to avoid it? Or are you concerned about it?00:52:00A: Yeah, I would say flakiness is again a smell for component that is really hard to test. And given that our project is a big project that was involved with like multi-threaded serialization and of course we had flakiness issues. By doing what I described before, trying to encapsulate the hard to test parts into utility methods or utility components, it was typically very easy to contain these flaky tests and it was basically always the same. So, it was the usual suspects that failed, and which is still annoying if you have a whole built pipeline after that relies on every test to pass. And then if your release build fails then that's super annoying but I mean of course we tried to contain that. For example we included like thresholds that said okay typically within 100 milliseconds you should get a response somewhere and then, depending on the system load, the system might be slower because something else happens at the same time, so it might take 200 milliseconds so we would just adopt the threshold accordingly. For some of the cases just retrying the test was also a worthwhile thing to do, because I mean in this case, I think it should not be the default. You should be very considerate about retrying a test. The point is that it might fail even if you retry it 5 times, then it still might fail 5times in the row.00:54:09A: [...] So retrying is not a good strategy, but at least if you rely on a passing bill, then I would say it's excusable. And. Yeah, I mean these are probably the two things that we do.00:54:53Q: [Conclusion of interview]